{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/(np.sum(np.exp(x), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(output, target):\n",
    "    return (target * np.log(output) + (1-target) * np.log(1-output)) * (-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build neuron network from scatch \n",
    "# w_ji, w_jk, w_ko: Weight at layer j, k ,o\n",
    "# i : input\n",
    "# t : target\n",
    "# l_m : maximum of loss, where to stop\n",
    "def build_NN(w_ij, w_jk, w_ko, i, t, l_m):\n",
    "    # set learning rate\n",
    "    l_rate = 0.01\n",
    "    \n",
    "    # Set temporary loss\n",
    "    L_sum = 1.0\n",
    "    \n",
    "    #Bias\n",
    "    b = 0.3\n",
    "    \n",
    "    # Count nums time of backpropagation\n",
    "    count = 0\n",
    "    \n",
    "    \n",
    "    # Keep update weight until loss is good enough\n",
    "    while True:\n",
    "        \n",
    "        ### Forward path\n",
    "        # Calculate input at j by dot product and add bias b = 0.3\n",
    "        j = np.dot(i, w_ij) + b\n",
    "        # output at layer j by apply activation function (Relu)\n",
    "        j_out = relu(j)\n",
    "        \n",
    "        # Calculate input at k by dot product and add bias b = 0.3\n",
    "        k = np.dot(j_out, w_jk) + b\n",
    "        # output at layer k by apply activation function (Relu)\n",
    "        k_out = relu(k)\n",
    "        \n",
    "        # Calculate input at o by dot product and add bias b = 0.3\n",
    "        o =  np.dot(k_out, w_ko) + b\n",
    "        \n",
    "        # output at layer o by apply activation function (softmax)\n",
    "        # Return softmax output (activation function)\n",
    "        o_out = softmax(o)\n",
    "        \n",
    "        # Calculate cross entropy loss function\n",
    "        L = cross_entropy(o_out, t)\n",
    "        \n",
    "        # Loss sum\n",
    "        L_sum = sum(L)\n",
    "        \n",
    "        if(L_sum <= l_m): # why break here ??? --> should cross-entropy always < 1.0\n",
    "            break\n",
    "        \n",
    "        # Otherwise do Backpropagation\n",
    "        \n",
    "        # Gradient of layer O\n",
    "        g_ko = k_out.reshape(3,1) * (((-1.0) * (t / o_out)) + ((1 - t) / (1 - o_out))) * (o_out - o_out * o_out)\n",
    "        \n",
    "        # Gradient of layer K\n",
    "        dL_dk = np.sum((((-1.0) * (t / o_out)) + ((1 - t) / (1 - o_out))) * (o_out - o_out * o_out) * w_ko, axis = 1)\n",
    "        g_jk = j_out.reshape(3,1) * dL_dk\n",
    "        \n",
    "        # Gradient of layer J\n",
    "        # Get diagonal of w_ij (weigth ij)\n",
    "        w_jj_kj = np.diag(w_jk)\n",
    "        g_ij = i.reshape(3,1) * (dL_dk * w_jj_kj)\n",
    "        \n",
    "        ### Set new weigth\n",
    "        w_ij = w_ij - (g_ij * l_rate)\n",
    "        w_jk = w_jk - (g_jk * l_rate)\n",
    "        w_ko = w_ko - (g_ko * l_rate)\n",
    "        \n",
    "        # Update count\n",
    "        count = count + 1\n",
    "    \n",
    "    return (w_ij, w_jk, w_ko, o_out, L_sum, count)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input at i\n",
    "i = np.array([0.1, 0.2, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.2, 0.3],\n",
       "       [0.3, 0.2, 0.7],\n",
       "       [0.4, 0.3, 0.9]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight from layer i to j\n",
    "w_ij = np.array([0.1 ,0.2, 0.3, 0.3, 0.2, 0.7, 0.4, 0.3, 0.9]).reshape(3,3)\n",
    "w_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.3, 0.5],\n",
       "       [0.3, 0.5, 0.7],\n",
       "       [0.6, 0.4, 0.8]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight from layer j to k\n",
    "w_jk = np.array([0.2, 0.3, 0.5, 0.3, 0.5, 0.7, 0.6, 0.4, 0.8]).reshape(3,3)\n",
    "w_jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.4, 0.8],\n",
       "       [0.3, 0.7, 0.2],\n",
       "       [0.5, 0.2, 0.9]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight from layer k to o\n",
    "w_ko = np.array([0.1, 0.4, 0.8, 0.3, 0.7, 0.2, 0.5, 0.2, 0.9]).reshape(3,3)\n",
    "w_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of target\n",
    "t = np.array([1.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "l_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum Loss\n",
    "l_m = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.09940437, 0.20408816, 0.32922893],\n",
       "        [0.29880874, 0.20817633, 0.75845786],\n",
       "        [0.39583058, 0.32861715, 1.10460249]]),\n",
       " array([[0.16126579, 0.42938604, 0.68898428],\n",
       "        [0.26636851, 0.61639099, 0.87037136],\n",
       "        [0.53769613, 0.63410452, 1.14424305]]),\n",
       " array([[ 0.6672003 ,  0.1980114 ,  0.4347883 ],\n",
       "        [ 0.97509169,  0.45492945, -0.23002114],\n",
       "        [ 1.53561836, -0.17588125,  0.24026289]]),\n",
       " array([0.99501222, 0.00221463, 0.00277314]),\n",
       " 0.009994345544873688,\n",
       " 927)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = build_NN(w_ij, w_jk, w_ko, i, t, l_m)\n",
    "my_out = result[3]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step By Step Explaination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.2, 0.3],\n",
       "       [0.3, 0.2, 0.7],\n",
       "       [0.4, 0.3, 0.9]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65, 0.57, 1.1 ])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate input at j by dot product and add bias b = 0.3\n",
    "b = 0.3\n",
    "j = np.dot(i, w_ij) + b\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65, 0.57, 1.1 ])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output at layer j by apply activation function (Relu)\n",
    "j_out = relu(j)\n",
    "j_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.3, 0.5],\n",
       "       [0.3, 0.5, 0.7],\n",
       "       [0.6, 0.4, 0.8]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.261, 1.22 , 1.904])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate input at k by dot product and add bias b = 0.3\n",
    "k = np.dot(j_out, w_jk) + b\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.261, 1.22 , 1.904])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output at layer k by apply activation function (Relu)\n",
    "k_out = relu(k)\n",
    "k_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.4, 0.8],\n",
       "       [0.3, 0.7, 0.2],\n",
       "       [0.5, 0.2, 0.9]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7441, 2.0392, 3.2664])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate input at o by dot product and add bias b = 0.3\n",
    "o =  np.dot(k_out, w_ko) + b\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14438319, 0.19394426, 0.66167255])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output at layer o by apply activation function (softmax)\n",
    "# Return softmax output (activation function)\n",
    "#o_out = np.exp(o)/(np.sum(np.exp(o), axis = 0))\n",
    "o_out = softmax(o)\n",
    "o_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.93528448, 0.21560238, 1.08374107])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate cross entropy loss function\n",
    "#L = (t * np.log(o_out) + (1-t) * np.log(1-o_out)) * (-1.0)\n",
    "L = cross_entropy(o_out, t)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2346279327622796"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find gradient for layer O by using backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{W}_{k_io_j}} = \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{O}_{out_j}} * \\frac{\\partial \\mathbf{O}_{out_j}}{\\partial \\mathbf{O}_{j}} * \\frac{\\partial \\mathbf{O}_{j}}{\\partial \\mathbf{W}_{k_io_j}} = (- \\frac{t_j}{\\mathbf{O}_{out_j}} + \\frac{1 - t_j}{1 - \\mathbf{O}_{out_j}}) * (\\mathbf{O}_{out_j} - \\mathbf{O}_{out_j} ^ 2) * \\mathbf{K}_{out_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14438319, 0.19394426, 0.66167255])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.261, 1.22 , 1.904])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient of layer O\n",
    "g_ko = k_out.reshape(3,1) * (((-1.0) * (t / o_out)) + ((1 - t) / (1 - o_out))) * (o_out - o_out * o_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0789328 ,  0.24456371,  0.83436909],\n",
       "       [-1.04385251,  0.236612  ,  0.80724051],\n",
       "       [-1.62909441,  0.36926987,  1.25982454]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.078932799024619"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1 * t[0] / o_out[0] + (1 - t[0]) / (1 - o_out[0])) * (o_out[0] - o_out[0] * o_out[0]) * k_out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find gradient for layer K by using backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{W}_{j_ik_j}} = \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{K}_{out_j}} * \\frac{\\partial \\mathbf{K}_{out_j}}{\\partial \\mathbf{K}_{j}} * \\frac{\\partial \\mathbf{K}_{j}}{\\partial \\mathbf{W}_{j_ik_j}} = \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{K}_{out_j}} * 1 * \\mathbf{J}_{out_i}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{K}_{out_j}} = \\frac{\\partial \\mathbf{L_1}}{\\partial \\mathbf{K}_{out_j}} + \\frac{\\partial \\mathbf{L_2}}{\\partial \\mathbf{K}_{out_j}} + \\frac{\\partial \\mathbf{L_3}}{\\partial \\mathbf{K}_{out_j}} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L_1}}{\\partial \\mathbf{K}_{out_1}} = \\frac{\\partial \\mathbf{L_1}}{\\partial \\mathbf{O}_{out_1}} * \\frac{\\partial \\mathbf{O}_{out_1}}{\\partial \\mathbf{O}_{1}} * \\frac{\\partial \\mathbf{O}_{1}}{\\partial \\mathbf{K}_{out_j}} = (- \\frac{t_1}{\\mathbf{O}_{out_1}} + \\frac{1 - t_1}{1 - \\mathbf{O}_{out_1}}) * (\\mathbf{O}_{out_1} - \\mathbf{O}_{out_1} ^ 2) * \\mathbf{W}_{k_jo_1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{K}_{out_j}} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52135406, 0.01141045, 0.20648574])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_dk = np.sum((((-1.0) * (t / o_out)) + ((1 - t) / (1 - o_out))) * (o_out - o_out * o_out) * w_ko, axis = 1)\n",
    "dL_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33888014, 0.00741679, 0.13421573],\n",
       "       [0.29717182, 0.00650396, 0.11769687],\n",
       "       [0.57348947, 0.01255149, 0.22713432]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradient of layer K\n",
    "g_jk = j_out.reshape(3,1) * dL_dk\n",
    "g_jk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find gradient for layer J by using backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{W}_{i_ij_j}} = \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{J}_{out_j}} * \\frac{\\partial \\mathbf{J}_{out_j}}{\\partial \\mathbf{J}_{j}} * \\frac{\\partial \\mathbf{J}_{j}}{\\partial \\mathbf{W}_{i_ij_j}} = \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{J}_{out_j}} * 1 * \\mathbf{I}_{out_i}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{J}_{out_j}} = \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{K}_{out_j}} * \\frac{\\partial \\mathbf{K}_{out_j}}{\\partial \\mathbf{K}_{j}} * \\frac{\\partial \\mathbf{K}_{j}}{\\partial \\mathbf{J}_{out_j}} = \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{K}_{out_j}} * 1 * \\mathbf{W}_{j_jk_j}\n",
    "\\end{align}\n",
    "\n",
    "This derivative is calculated as above\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{K}_{out_j}} \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52135406, 0.01141045, 0.20648574])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_dk = np.sum((((-1.0) * (t / o_out)) + ((1 - t) / (1 - o_out))) * (o_out - o_out * o_out) * w_ko, axis = 1)\n",
    "dL_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.2, 0.9])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get diagonal of w_ij (weigth ij)\n",
    "w_jj_kj = np.diag(w_ij)\n",
    "w_jj_kj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00521354, 0.00022821, 0.01858372],\n",
       "       [0.01042708, 0.00045642, 0.03716743],\n",
       "       [0.03649478, 0.00159746, 0.13008602]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradient of layer J\n",
    "g_ij = i.reshape(3,1) * (dL_dk * w_jj_kj)\n",
    "g_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
